{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ba9c0b-1b17-49a5-80a7-170493c62e59",
   "metadata": {},
   "source": [
    "# Imports for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b185ad4-423c-4d48-8541-76b2989747ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pathlib\n",
    "import itertools\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from  keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print ('modules loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f21f9-b296-4a6e-9cea-706a1507d431",
   "metadata": {},
   "source": [
    "# Required functions for data path + data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64371b94-fbb9-4afe-b8cc-5b53f2e456b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_paths(data_dir):\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "\n",
    "    folds = os.listdir(data_dir)\n",
    "    for fold in folds:\n",
    "        foldpath = os.path.join(data_dir, fold)\n",
    "        filelist = os.listdir(foldpath)\n",
    "        for file in filelist:\n",
    "            fpath = os.path.join(foldpath, file)\n",
    "            filepaths.append(fpath)\n",
    "            labels.append(fold)\n",
    "\n",
    "    return filepaths, labels\n",
    "def define_df(files, classes):\n",
    "    Fseries = pd.Series(files, name= 'filepaths')\n",
    "    Lseries = pd.Series(classes, name='labels')\n",
    "    return pd.concat([Fseries, Lseries], axis= 1)\n",
    "def split_data(data_dir_train, data_dir_val, data_dir_test):\n",
    "    files, classes = define_paths(data_dir_train)\n",
    "    files_val, classes_val = define_paths(data_dir_val)\n",
    "    files_test, classes_test = define_paths(data_dir_test)\n",
    "    \n",
    "    train_df = define_df(files, classes)\n",
    "    valid_df = define_df(files_val, classes_val)\n",
    "    test_df = define_df(files_test, classes_test)\n",
    "    \n",
    "    return train_df, valid_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e791d75-e579-49f8-86b2-213e98503af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gens (train_df, valid_df, test_df, batch_size):\n",
    "    img_size = (224, 224)\n",
    "    channels = 3\n",
    "    color = 'rgb'\n",
    "    img_shape = (img_size[0], img_size[1], channels)\n",
    "    ts_length = len(test_df)\n",
    "    test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "    test_steps = ts_length // test_batch_size\n",
    "    def scalar(img):\n",
    "        return img\n",
    "\n",
    "    tr_gen = ImageDataGenerator(preprocessing_function= scalar, horizontal_flip= True)\n",
    "    ts_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
    "    print(train_df)\n",
    "    train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                        color_mode= color, shuffle= True, batch_size= batch_size)\n",
    "\n",
    "    valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                        color_mode= color, shuffle= True, batch_size= batch_size)\n",
    "    test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                        color_mode= color, shuffle= False, batch_size= test_batch_size)\n",
    "\n",
    "    return train_gen, valid_gen, test_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a548a02-8e68-49b7-9d1e-ad38e5469b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(gen):\n",
    "    g_dict = gen.class_indices        \n",
    "    classes = list(g_dict.keys())     \n",
    "    images, labels = next(gen)       \n",
    "    length = len(labels)   \n",
    "    sample = min(length, 25)    \n",
    "\n",
    "    plt.figure(figsize= (20, 20))\n",
    "    for i in range(sample):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image = images[i] / 255  \n",
    "        image_size = image.shape\n",
    "        plt.imshow(image)\n",
    "        index = np.argmax(labels[i])  \n",
    "        class_name = classes[index]   \n",
    "        plt.title(class_name, color= 'blue', fontsize= 12)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6e4b25c-5715-4502-9db2-f5f72f8ac384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model, patience, stop_patience, threshold, factor, batches, epochs, ask_epoch):\n",
    "        super(MyCallback, self).__init__()\n",
    "        self.model = model\n",
    "        self.patience = patience \n",
    "        self.stop_patience = stop_patience \n",
    "        self.threshold = threshold \n",
    "        self.factor = factor \n",
    "        self.batches = batches \n",
    "        self.epochs = epochs\n",
    "        self.ask_epoch = ask_epoch\n",
    "        self.ask_epoch_initial = ask_epoch \n",
    "        \n",
    "        self.count = 0\n",
    "        self.stop_count = 0\n",
    "        self.best_epoch = 1   \n",
    "        self.initial_lr = float(tf.keras.backend.get_value(model.optimizer.lr)) \n",
    "        self.highest_tracc = 0.0 \n",
    "        self.lowest_vloss = np.inf \n",
    "        self.best_weights = self.model.get_weights()\n",
    "        self.initial_weights = self.model.get_weights()  \n",
    "    def on_train_begin(self, logs= None):\n",
    "        msg = 'Do you want model asks you to halt the training [y/n] ?'\n",
    "        print(msg)\n",
    "        ans = input('')\n",
    "        if ans in ['Y', 'y']:\n",
    "            self.ask_permission = 1\n",
    "        elif ans in ['N', 'n']:\n",
    "            self.ask_permission = 0\n",
    "\n",
    "        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
    "        print(msg)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "\n",
    "    def on_train_end(self, logs= None):\n",
    "        stop_time = time.time()\n",
    "        tr_duration = stop_time - self.start_time\n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "\n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print(msg)\n",
    "        self.model.set_weights(self.best_weights)\n",
    "\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs= None):\n",
    "        acc = logs.get('accuracy') * 100\n",
    "        loss = logs.get('loss')\n",
    "        msg = '{0:20s}processing batch {1:} of {2:5s}-   accuracy=  {3:5.3f}   -   loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
    "        print(msg, '\\r', end= '')\n",
    "\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs= None):\n",
    "        self.ep_start = time.time()\n",
    "    def on_epoch_end(self, epoch, logs= None):\n",
    "        ep_end = time.time()\n",
    "        duration = ep_end - self.ep_start\n",
    "\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.lr)) \n",
    "        current_lr = lr\n",
    "        acc = logs.get('accuracy') \n",
    "        v_acc = logs.get('val_accuracy')  \n",
    "        loss = logs.get('loss')  \n",
    "        v_loss = logs.get('val_loss') \n",
    "        if acc < self.threshold: \n",
    "            monitor = 'accuracy'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "            else:\n",
    "                pimprov = (acc - self.highest_tracc ) * 100 / self.highest_tracc \n",
    "            if acc > self.highest_tracc: \n",
    "                self.highest_tracc = acc \n",
    "                self.best_weights = self.model.get_weights() \n",
    "                self.count = 0 \n",
    "                self.stop_count = 0 \n",
    "                if v_loss < self.lowest_vloss:\n",
    "                    self.lowest_vloss = v_loss\n",
    "                self.best_epoch = epoch + 1 \n",
    "\n",
    "            else:\n",
    "                if self.count >= self.patience - 1: \n",
    "                    lr = lr * self.factor \n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) \n",
    "                    self.count = 0 \n",
    "                    self.stop_count = self.stop_count + 1 \n",
    "                    self.count = 0 \n",
    "                    if v_loss < self.lowest_vloss:\n",
    "                        self.lowest_vloss = v_loss\n",
    "                else:\n",
    "                    self.count = self.count + 1 \n",
    "        else: \n",
    "            monitor = 'val_loss'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "\n",
    "            \n",
    "            else:\n",
    "                pimprov = (self.lowest_vloss - v_loss ) * 100 / self.lowest_vloss\n",
    "\n",
    "            if v_loss < self.lowest_vloss: \n",
    "                self.lowest_vloss = v_loss \n",
    "                self.best_weights = self.model.get_weights() \n",
    "                self.count = 0 \n",
    "                self.stop_count = 0\n",
    "                self.best_epoch = epoch + 1 \n",
    "\n",
    "            else: \n",
    "                if self.count >= self.patience - 1: \n",
    "                    lr = lr * self.factor \n",
    "                    self.stop_count = self.stop_count + 1 \n",
    "                    self.count = 0 \n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) \n",
    "\n",
    "                else:\n",
    "                    self.count = self.count + 1 \n",
    "\n",
    "                if acc > self.highest_tracc:\n",
    "                    self.highest_tracc = acc\n",
    "\n",
    "        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc * 100:^9.3f}{v_loss:^9.5f}{v_acc * 100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
    "        print(msg)\n",
    "\n",
    "        if self.stop_count > self.stop_patience - 1: \n",
    "            msg = f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
    "            print(msg)\n",
    "            self.model.stop_training = True \n",
    "\n",
    "        else:\n",
    "            if self.ask_epoch != None and self.ask_permission != 0:\n",
    "                if epoch + 1 >= self.ask_epoch:\n",
    "                    msg = 'enter H to halt training or an integer for number of epochs to run then ask again'\n",
    "                    print(msg)\n",
    "\n",
    "                    ans = input('')\n",
    "                    if ans == 'H' or ans == 'h':\n",
    "                        msg = f'training has been halted at epoch {epoch + 1} due to user input'\n",
    "                        print(msg)\n",
    "                        self.model.stop_training = True \n",
    "\n",
    "                    else:\n",
    "                        try:\n",
    "                            ans = int(ans)\n",
    "                            self.ask_epoch += ans\n",
    "                            msg = f' training will continue until epoch {str(self.ask_epoch)}'\n",
    "                            print(msg)\n",
    "                            msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor', '% Improv', 'Duration')\n",
    "                            print(msg)\n",
    "\n",
    "                        except Exception:\n",
    "                            print('Invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8089f542-769b-43e3-b39d-bf352d90b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(hist):\n",
    "    tr_acc = hist.history['accuracy']\n",
    "    tr_loss = hist.history['loss']\n",
    "    val_acc = hist.history['val_accuracy']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    index_loss = np.argmin(val_loss)\n",
    "    val_lowest = val_loss[index_loss]\n",
    "    index_acc = np.argmax(val_acc)\n",
    "    acc_highest = val_acc[index_acc]\n",
    "    Epochs = [i+1 for i in range(len(tr_acc))]\n",
    "    loss_label = f'best epoch= {str(index_loss + 1)}'\n",
    "    acc_label = f'best epoch= {str(index_acc + 1)}'\n",
    "    \n",
    "    plt.figure(figsize= (20, 8))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
    "    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
    "    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
    "    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
    "    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c987564a-dbcf-4e92-bbcb-51a76aa87a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize= False, title= 'Confusion Matrix', cmap= plt.cm.Blues):\n",
    "    plt.figure(figsize= (10, 10))\n",
    "    plt.imshow(cm, interpolation= 'nearest', cmap= cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation= 45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis= 1)[:, np.newaxis]\n",
    "        print('Normalized Confusion Matrix')\n",
    "    else:\n",
    "        print('Confusion Matrix, Without Normalization')\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig('cropped_results/cropped_conf_matrix.png')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2187f-d1cc-471f-ba3a-90ca494ac1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'Your path train dir'\n",
    "val_dir = 'Your path valid dir'\n",
    "test_dir = 'Your path test dir'\n",
    "try:\n",
    "    train_df, valid_df, test_df = split_data(train_dir, val_dir, test_dir)\n",
    "    batch_size = 8\n",
    "    train_gen, valid_gen, test_gen = create_gens(train_df, valid_df, test_df, batch_size)\n",
    "\n",
    "except:\n",
    "    print('Invalid Input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d26b206-d28e-4863-b3dd-a3322076b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "915191d7-9a5e-464e-9d61-a6ead78ce7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " densenet169 (Functional)    (None, 1664)              12642880  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 3330      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,646,210\n",
      "Trainable params: 12,487,810\n",
      "Non-trainable params: 158,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "class_count = len(list(train_gen.class_indices.keys()))\n",
    "\n",
    "base_model = tf.keras.applications.densenet.DenseNet169(include_top= False, weights= None, input_shape= img_shape, pooling= 'max')\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Dense(class_count, activation= 'softmax')\n",
    "])\n",
    "model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c79c394-97ce-4f25-8282-f5c6e1539dca",
   "metadata": {},
   "source": [
    "## From here training the original model starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b21068d-2a2c-4628-96eb-1b9ae8ba1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32   \n",
    "epochs = 50   \n",
    "patience = 5   \n",
    "stop_patience = 10   \n",
    "threshold = 0.9   \n",
    "factor = 0.5   \n",
    "ask_epoch = 5   \n",
    "batches = int(np.ceil(len(train_gen.labels) / batch_size))    \n",
    "callbacks = [MyCallback(model= model, patience= patience, stop_patience= stop_patience, threshold= threshold,\n",
    "            factor= factor, batches= batches, epochs= epochs, ask_epoch= ask_epoch )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36caa13-a572-4e94-b5f3-75c0bd1aa9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x= train_gen, epochs= epochs, verbose= 0, callbacks= callbacks,\n",
    "                    validation_data= valid_gen, validation_steps= None, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443109a-4f34-4fb0-9a0c-c39533e43340",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54e795c-3591-4841-96ef-e4aac5189814",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_length = len(test_df)\n",
    "test_batch_size = test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "test_steps = ts_length // test_batch_size\n",
    "\n",
    "train_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\n",
    "valid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\n",
    "test_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train Accuracy: \", train_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Validation Loss: \", valid_score[0])\n",
    "print(\"Validation Accuracy: \", valid_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6f9ed-e7e0-40e4-9434-b1147327989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c67b4dc0-44d4-472d-bc40-f1170a87b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"Your_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dcfb48-29d8-4ced-8439-2764b207c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_gen)\n",
    "y_pred = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee100d-f03e-47b5-b431-9565dd474ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dict = test_gen.class_indices\n",
    "classes = list(g_dict.keys())\n",
    "\n",
    "cm = confusion_matrix(test_gen.classes, y_pred)\n",
    "plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix')\n",
    "\n",
    "print(classification_report(test_gen.classes, y_pred, target_names= classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea8e57-2281-45e6-8132-24343d2578b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical_paper",
   "language": "python",
   "name": "medical_paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
